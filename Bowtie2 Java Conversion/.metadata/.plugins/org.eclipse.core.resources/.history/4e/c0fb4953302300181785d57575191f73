package com.uwb.bt2j.indexer.blockwise;

import org.omg.CORBA_2_3.portable.OutputStream;

import com.uwb.bt2j.indexer.RandomSource;
import com.uwb.bt2j.indexer.types.ELList;
import com.uwb.bt2j.indexer.types.EList;

import javafx.util.Pair;

public class KarkkainenBlockwiseSA extends BlockwiseSA{
	
	class BinarySortingParam {
		String t;
		EList<Long> sampleSuffs;
		EList<Long> bucketSzs;
		EList<Long> bucketReps;
		int begin;
		int end;
	}
	
	private EList<Long> _sampleSuffs;
	private int _nthreads;
	private long _itrBucketIdx;
	private long _cur;
	private int _dcV;
	private boolean _built;
	private RandomSource _randomSrc;
	private String _base_fname;
	private boolean _bigEndian;
	//private EList<Thread>
	private EList<Pair<KarkkainenBlockwiseSA, Integer>> _tParams;
	private ELList<Long> _itrBuckets;
	private boolean _done;

	public KarkkainenBlockwiseSA(String __text, long __bucketSz, int __nthreads, int __dcV, int __seed, boolean __sanityCheck, boolean __passMemExc,
			boolean __verbose, String base_fname, OutputStream __logger) {
		super(__text, __bucketSz, __sanityCheck, __passMemExc, __verbose, __logger);
		_sampleSuffs = new EList(2);
		_nthreads = __nthreads;
		_itrBucketIdx = 0;
		_cur = 0;
		_dcV = __dcV;
		_dc = 2;
		_built = false;
		_base_fname = base_fname;
		_bigEndian = false;
	}
	
	public static int simulateAllocs(String text, int bucketSz) {
		 int len = text.length();
	        // _sampleSuffs and _itrBucket are in memory at the peak
	        int bsz = bucketSz;
	        int sssz = (int) (len / Long.max(bucketSz-1, 1));
	        long[] tmp = new long[bsz + sssz + (1024 * 1024 /*out of caution*/)];
	        return bsz;
	}

	public long nextSuffix() {
		// Launch threads if not
				if(this._nthreads > 1) {
		
					if(_threads.size() == 0) {
		                _done = _sampleSuffs.size() + 1; 
		                for (int i = 0; i < _sampleSuffs.size() + 1; i++) {
		                    _done.get()[i] = false;
		                }
						_itrBuckets.resize(this._nthreads);
						_tparams.resize(this._nthreads);
						for(int tid = 0; tid < this._nthreads; tid++) {
							_tparams[tid].first = this;
							_tparams[tid].second = tid;
							_threads.push_back(new tthread::thread(nextBlock_Worker, (void*)&_tparams[tid]));
						}
					}
				}
				if(this._itrPushedBackSuffix != OFF_MASK) {
					TIndexOffU tmp = this._itrPushedBackSuffix;
					this._itrPushedBackSuffix = OFF_MASK;
					return tmp;
				}
				while(this._itrBucketPos >= this._itrBucket.size() ||
				      this._itrBucket.size() == 0)
				{
					if(!hasMoreBlocks()) {
						throw out_of_range("No more suffixes");
					}
					if(this._nthreads == 1) {
						nextBlock((int)_cur);
						_cur++;
					} else {
						while(!_done.get()[this._itrBucketIdx]) {
							SLEEP(1);
						}
						// Read suffixes from a file
						std::ostringstream number; number << this._itrBucketIdx;
						const string fname = _base_fname + "." + number.str() + ".sa";
						ifstream sa_file(fname.c_str(), ios::binary);
						if(!sa_file.good()) {
							cerr << "Could not open file for reading a suffix array: \"" << fname << "\"" << endl;
							throw 1;
						}
						int numSAs = readU<TIndexOffU>(sa_file, _bigEndian);
						this._itrBucket.resizeExact(numSAs);
						for(int i = 0; i < numSAs; i++) {
							this._itrBucket[i] = readU<TIndexOffU>(sa_file, _bigEndian);
						}
						sa_file.close();
						std::remove(fname.c_str());
					}
					this._itrBucketIdx++;
					this._itrBucketPos = 0;
				}
				return this._itrBucket[this._itrBucketPos++];
	}
	
	public int dcV() {
		return _dcV;
	}
	
	public static void BinarySorting_worker() {
		 BinarySortingParam<TStr>* param = (BinarySortingParam<TStr>*)vp;
		    const TStr& t = *(param->t);
		    size_t len = t.length();
		    const EList<TIndexOffU>& sampleSuffs = *(param->sampleSuffs);
		    EList<TIndexOffU>& bucketSzs = param->bucketSzs;
		    EList<TIndexOffU>& bucketReps = param->bucketReps;
		    ASSERT_ONLY(size_t numBuckets = bucketSzs.size());
		    size_t begin = param->begin;
		    size_t end = param->end;
		    // Iterate through every suffix in the text, determine which
		    // bucket it falls into by doing a binary search across the
		    // sorted list of samples, and increment a counter associated
		    // with that bucket.  Also, keep one representative for each
		    // bucket so that we can split it later.  We loop in ten
		    // stretches so that we can print out a helpful progress
		    // message.  (This step can take a long time.)
		    for(TIndexOffU i = (TIndexOffU)begin; i < end && i < len; i++) {
		        TIndexOffU r = binarySASearch(t, i, sampleSuffs);
		        if(r == std::numeric_limits<TIndexOffU>::max()) continue; // r was one of the samples
		        assert_lt(r, numBuckets);
		        bucketSzs[r]++;
		        assert_lt(bucketSzs[r], len);
		        if(bucketReps[r] == OFF_MASK || (i & 100) == 0) {
		            bucketReps[r] = i; // clobbers previous one, but that's OK
		        }
		    }
	}
	
	public static void nextBlockWorker() {
		pair<KarkkainenBlockwiseSA*, int> param = *(pair<KarkkainenBlockwiseSA*, int>*)vp;
        KarkkainenBlockwiseSA* sa = param.first;
        int tid = param.second;
        while(true) {
            size_t cur = 0;
            {
                ThreadSafe ts(sa->_mutex);
                cur = sa->_cur;
                if(cur > sa->_sampleSuffs.size()) break;
                sa->_cur++;
            }
            sa->nextBlock((int)cur, tid);
            // Write suffixes into a file
            std::ostringstream number; number << cur;
            const string fname = sa->_base_fname + "." + number.str() + ".sa";
            ofstream sa_file(fname.c_str(), ios::binary);
            if(!sa_file.good()) {
                cerr << "Could not open file for writing a reference graph: \"" << fname << "\"" << endl;
                throw 1;
            }
            const EList<TIndexOffU>& bucket = sa->_itrBuckets[tid];
            writeU<TIndexOffU>(sa_file, (TIndexOffU)bucket.size(), sa->_bigEndian);
            for(size_t i = 0; i < bucket.size(); i++) {
                writeU<TIndexOffU>(sa_file, bucket[i], sa->_bigEndian);
            }
            sa_file.close();
            sa->_itrBuckets[tid].clear();
            sa->_done.get()[cur] = true;
        }
	}
	
	protected void reset() {
		if(!_built) {
			build();
		}
		_cur = 0;
	}
	
	protected boolean isReset() {
		return _cur == 0;
	}
	
	public void nextBlock(int cur_block, int tid) {
		EList<TIndexOffU>& bucket = (this->_nthreads > 1 ? this->_itrBuckets[tid] : this->_itrBucket);
	    {
	        ThreadSafe ts(_mutex);
	        VMSG_NL("Getting block " << (cur_block+1) << " of " << _sampleSuffs.size()+1);
	    }
	    assert(_built);
	    assert_gt(_dcV, 3);
	    assert_leq(cur_block, (int)_sampleSuffs.size());
	    const TStr& t = this->text();
	    TIndexOffU len = (TIndexOffU)t.length();
	    // Set up the bucket
	    bucket.clear();
	    TIndexOffU lo = OFF_MASK, hi = OFF_MASK;
	    if(_sampleSuffs.size() == 0) {
	        // Special case: if _sampleSuffs is 0, then multikey-quicksort
	        // everything
	        {
	            ThreadSafe ts(_mutex);
	            VMSG_NL("  No samples; assembling all-inclusive block");
	        }
	        assert_eq(0, cur_block);
	        try {
	            if(bucket.capacity() < this->bucketSz()) {
	                bucket.reserveExact(len+1);
	            }
	            bucket.resize(len);
	            for(TIndexOffU i = 0; i < len; i++) {
	                bucket[i] = i;
	            }
	        } catch(bad_alloc &e) {
	            if(this->_passMemExc) {
	                throw e; // rethrow immediately
	            } else {
	                cerr << "Could not allocate a master suffix-array block of " << ((len+1) * 4) << " bytes" << endl
	                << "Please try using a larger number of blocks by specifying a smaller --bmax or" << endl
	                << "a larger --bmaxdivn" << endl;
	                throw 1;
	            }
	        }
	    } else {
	        try {
	            {
	                ThreadSafe ts(_mutex);
	                VMSG_NL("  Reserving size (" << this->bucketSz() << ") for bucket " << (cur_block+1));
	            }
	            // BTL: Add a +100 fudge factor; there seem to be instances
	            // where a bucket ends up having one more elt than bucketSz()
	            if(bucket.size() < this->bucketSz()+100) {
	                bucket.reserveExact(this->bucketSz()+100);
	            }
	        } catch(bad_alloc &e) {
	            if(this->_passMemExc) {
	                throw e; // rethrow immediately
	            } else {
	                cerr << "Could not allocate a suffix-array block of " << ((this->bucketSz()+1) * 4) << " bytes" << endl;
	                cerr << "Please try using a larger number of blocks by specifying a smaller --bmax or" << endl
	                << "a larger --bmaxdivn" << endl;
	                throw 1;
	            }
	        }
	        // Select upper and lower bounds from _sampleSuffs[] and
	        // calculate the Z array up to the difference-cover periodicity
	        // for both.  Be careful about first/last buckets.
	        EList<TIndexOffU> zLo(EBWTB_CAT), zHi(EBWTB_CAT);
	        assert_geq(cur_block, 0);
	        assert_leq((size_t)cur_block, _sampleSuffs.size());
	        bool first = (cur_block == 0);
	        bool last  = ((size_t)cur_block == _sampleSuffs.size());
	        try {
	            // Timer timer(cout, "  Calculating Z arrays time: ", this->verbose());
	            {
	                ThreadSafe ts(_mutex);
	                VMSG_NL("  Calculating Z arrays for bucket " << (cur_block+1));
	            }
	            if(!last) {
	                // Not the last bucket
	                assert_lt(cur_block, (int)_sampleSuffs.size());
	                hi = _sampleSuffs[cur_block];
	                zHi.resizeExact(_dcV);
	                zHi.fillZero();
	                assert_eq(zHi[0], 0);
	                calcZ(t, hi, zHi, this->verbose(), this->sanityCheck());
	            }
	            if(!first) {
	                // Not the first bucket
	                assert_gt(cur_block, 0);
	                assert_leq(cur_block, (int)_sampleSuffs.size());
	                lo = _sampleSuffs[cur_block-1];
	                zLo.resizeExact(_dcV);
	                zLo.fillZero();
	                assert_gt(_dcV, 3);
	                assert_eq(zLo[0], 0);
	                calcZ(t, lo, zLo, this->verbose(), this->sanityCheck());
	            }
	        } catch(bad_alloc &e) {
	            if(this->_passMemExc) {
	                throw e; // rethrow immediately
	            } else {
	                cerr << "Could not allocate a z-array of " << (_dcV * 4) << " bytes" << endl;
	                cerr << "Please try using a larger number of blocks by specifying a smaller --bmax or" << endl
	                << "a larger --bmaxdivn" << endl;
	                throw 1;
	            }
	        }
	        
	        // This is the most critical loop in the algorithm; this is where
	        // we iterate over all suffixes in the text and pick out those that
	        // fall into the current bucket.
	        //
	        // This loop is based on the SMALLERSUFFIXES function outlined on
	        // p7 of the "Fast BWT" paper
	        //
	        int64_t kHi = -1, kLo = -1;
	        int64_t jHi = -1, jLo = -1;
	        bool kHiSoft = false, kLoSoft = false;
	        assert_eq(0, bucket.size());
	        {
	            // Timer timer(cout, "  Block accumulator loop time: ", this->verbose());
	            {
	                ThreadSafe ts(_mutex);
	                VMSG_NL("  Entering block accumulator loop for bucket " << (cur_block+1) << ":");
	            }
	            TIndexOffU lenDiv10 = (len + 9) / 10;
	            for(TIndexOffU iten = 0, ten = 0; iten < len; iten += lenDiv10, ten++) {
	                TIndexOffU itenNext = iten + lenDiv10;
	                {
	                    ThreadSafe ts(_mutex);
	                    if(ten > 0) VMSG_NL("  bucket " << (cur_block+1) << ": " << (ten * 10) << "%");
	                }
	                for(TIndexOffU i = iten; i < itenNext && i < len; i++) {
	                    assert_lt(jLo, (TIndexOff)i); assert_lt(jHi, (TIndexOff)i);
	                    // Advance the upper-bound comparison by one character
	                    if(i == hi || i == lo) continue; // equal to one of the bookends
	                    if(hi != OFF_MASK && !suffixCmp(hi, i, jHi, kHi, kHiSoft, zHi)) {
	                        continue; // not in the bucket
	                    }
	                    if(lo != OFF_MASK && suffixCmp(lo, i, jLo, kLo, kLoSoft, zLo)) {
	                        continue; // not in the bucket
	                    }
	                    // In the bucket! - add it
	                    assert_lt(i, len);
	                    try {
	                        bucket.push_back(i);
	                    } catch(bad_alloc &e) {
	                        cerr << "Could not append element to block of " << ((bucket.size()) * OFF_SIZE) << " bytes" << endl;
	                        if(this->_passMemExc) {
	                            throw e; // rethrow immediately
	                        } else {
	                            cerr << "Please try using a larger number of blocks by specifying a smaller --bmax or" << endl
	                            << "a larger --bmaxdivn" << endl;
	                            throw 1;
	                        }
	                    }
	                    // Not necessarily true; we allow overflowing buckets
	                    // since we can't guarantee that a good set of sample
	                    // suffixes can be found in a reasonable amount of time
	                    //assert_lt(bucket.size(), this->bucketSz());
	                }
	            } // end loop over all suffixes of t
	            {
	                ThreadSafe ts(_mutex);
	                VMSG_NL("  bucket " << (cur_block+1) << ": 100%");
	            }
	        }
	    } // end else clause of if(_sampleSuffs.size() == 0)
	    // Sort the bucket
	    if(bucket.size() > 0) {
	        Timer timer(cout, "  Sorting block time: ", this->verbose());
	        {
	            ThreadSafe ts(_mutex);
	            VMSG_NL("  Sorting block of length " << bucket.size() << " for bucket " << (cur_block+1));
	        }
	        this->qsort(bucket);
	    }
	    if(hi != OFF_MASK) {
	        // Not the final bucket; throw in the sample on the RHS
	        bucket.push_back(hi);
	    } else {
	        // Final bucket; throw in $ suffix
	        bucket.push_back(len);
	    }
	    {
	        ThreadSafe ts(_mutex);
	        VMSG_NL("Returning block of " << bucket.size() << " for bucket " << (cur_block+1));
	    }
	}
	
	private void build() {
		// Calculate difference-cover sample
        if(_dcV != 0) {
            _dc.init(new TDC(this->text(), _dcV, this->verbose(), this->sanityCheck()));
            _dc.get()->build(this->_nthreads);
        }
        // Calculate sample suffixes
        if(this->bucketSz() <= this->text().length()) {
            VMSG_NL("Building samples");
            buildSamples();
        } else {
            VMSG_NL("Skipping building samples since text length " <<
                    this->text().length() << " is less than bucket size: " <<
                    this->bucketSz());
        }
        _built = true;
	}
	
	private void buildSamples() {
		const TStr& t = this->text();
	    TIndexOffU bsz = this->bucketSz()-1; // subtract 1 to leave room for sample
	    size_t len = this->text().length();
	    // Prepare _sampleSuffs array
	    _sampleSuffs.clear();
	    TIndexOffU numSamples = (TIndexOffU)((len/bsz)+1)<<1; // ~len/bsz x 2
	    assert_gt(numSamples, 0);
	    VMSG_NL("Reserving space for " << numSamples << " sample suffixes");
	    if(this->_passMemExc) {
	        _sampleSuffs.resizeExact(numSamples);
	        // Randomly generate samples.  Allow duplicates for now.
	        VMSG_NL("Generating random suffixes");
	        for(size_t i = 0; i < numSamples; i++) {
	#ifdef BOWTIE_64BIT_INDEX
	            _sampleSuffs[i] = (TIndexOffU)(_randomSrc.nextU64() % len);
	#else
	            _sampleSuffs[i] = (TIndexOffU)(_randomSrc.nextU32() % len);
	#endif
	        }
	    } else {
	        try {
	            _sampleSuffs.resizeExact(numSamples);
	            // Randomly generate samples.  Allow duplicates for now.
	            VMSG_NL("Generating random suffixes");
	            for(size_t i = 0; i < numSamples; i++) {
	#ifdef BOWTIE_64BIT_INDEX
	                _sampleSuffs[i] = (TIndexOffU)(_randomSrc.nextU64() % len);
	#else
	                _sampleSuffs[i] = (TIndexOffU)(_randomSrc.nextU32() % len);
	#endif
	            }
	        } catch(bad_alloc &e) {
	            if(this->_passMemExc) {
	                throw e; // rethrow immediately
	            } else {
	                cerr << "Could not allocate sample suffix container of " << (numSamples * OFF_SIZE) << " bytes." << endl
	                << "Please try using a smaller number of blocks by specifying a larger --bmax or" << endl
	                << "a smaller --bmaxdivn" << endl;
	                throw 1;
	            }
	        }
	    }
	    // Remove duplicates; very important to do this before the call to
	    // mkeyQSortSuf so that it doesn't try to calculate lexicographical
	    // relationships between very long, identical strings, which takes
	    // an extremely long time in general, and causes the stack to grow
	    // linearly with the size of the input
	    {
	        Timer timer(cout, "QSorting sample offsets, eliminating duplicates time: ", this->verbose());
	        VMSG_NL("QSorting " << _sampleSuffs.size() << " sample offsets, eliminating duplicates");
	        _sampleSuffs.sort();
	        size_t sslen = _sampleSuffs.size();
	        for(size_t i = 0; i < sslen-1; i++) {
	            if(_sampleSuffs[i] == _sampleSuffs[i+1]) {
	                _sampleSuffs.erase(i--);
	                sslen--;
	            }
	        }
	    }
	    // Multikey quicksort the samples
	    {
	        Timer timer(cout, "  Multikey QSorting samples time: ", this->verbose());
	        VMSG_NL("Multikey QSorting " << _sampleSuffs.size() << " samples");
	        this->qsort(_sampleSuffs);
	    }
	    // Calculate bucket sizes
	    VMSG_NL("Calculating bucket sizes");
	    int limit = 5;
	    // Iterate until all buckets are less than
	    while(--limit >= 0) {
	        TIndexOffU numBuckets = (TIndexOffU)_sampleSuffs.size()+1;
	#ifdef WITH_TBB
		tbb::task_group tbb_grp;
	#else
	        AutoArray<tthread::thread*> threads(this->_nthreads);
	#endif
	        EList<BinarySortingParam<TStr> > tparams;
	        tparams.resize(this->_nthreads);
	        for(int tid = 0; tid < this->_nthreads; tid++) {
	            // Calculate bucket sizes by doing a binary search for each
	            // suffix and noting where it lands
	            try {
	                // Allocate and initialize containers for holding bucket
	                // sizes and representatives.
	                tparams[tid].bucketSzs.resizeExact(numBuckets);
	                tparams[tid].bucketReps.resizeExact(numBuckets);
	                tparams[tid].bucketSzs.fillZero();
	                tparams[tid].bucketReps.fill(OFF_MASK);
	            } catch(bad_alloc &e) {
	                if(this->_passMemExc) {
	                    throw e; // rethrow immediately
	                } else {
	                    cerr << "Could not allocate sizes, representatives (" << ((numBuckets*8)>>10) << " KB) for blocks." << endl
	                    << "Please try using a smaller number of blocks by specifying a larger --bmax or a" << endl
	                    << "smaller --bmaxdivn." << endl;
	                    throw 1;
	                }
	            }
	            tparams[tid].t = &t;
	            tparams[tid].sampleSuffs = &_sampleSuffs;
	            tparams[tid].begin = (tid == 0 ? 0 : len / this->_nthreads * tid);
	            tparams[tid].end = (tid + 1 == this->_nthreads ? len : len / this->_nthreads * (tid + 1));
	            if(this->_nthreads == 1) {
	                BinarySorting_worker<TStr>((void*)&tparams[tid]);
	            } else {
	#ifdef WITH_TBB
	        			tbb_grp.run(BinarySorting_worker<TStr>(((void*)&tparams[tid])));
			        }
	        }
	      	tbb_grp.wait();
	#else
	             threads[tid] = new tthread::thread(BinarySorting_worker<TStr>, (void*)&tparams[tid]);
	            }
	        }
	        
	        if(this->_nthreads > 1) {
	            for (int tid = 0; tid < this->_nthreads; tid++) {
	                threads[tid]->join();
	            }
	        }
	#endif
	        
	        EList<TIndexOffU>& bucketSzs = tparams[0].bucketSzs;
	        EList<TIndexOffU>& bucketReps = tparams[0].bucketReps;
	        for(int tid = 1; tid < this->_nthreads; tid++) {
	            for(size_t j = 0; j < numBuckets; j++) {
	                bucketSzs[j] += tparams[tid].bucketSzs[j];
	                if(bucketReps[j] == OFF_MASK) {
	                    bucketReps[j] = tparams[tid].bucketReps[j];
	                }
	            }
	        }
	        // Check for large buckets and mergeable pairs of small buckets
	        // and split/merge as necessary
	        TIndexOff added = 0;
	        TIndexOff merged = 0;
	        assert_eq(bucketSzs.size(), numBuckets);
	        assert_eq(bucketReps.size(), numBuckets);
	        {
	            Timer timer(cout, "  Splitting and merging time: ", this->verbose());
	            VMSG_NL("Splitting and merging");
	            for(TIndexOffU i = 0; i < numBuckets; i++) {
	                TIndexOffU mergedSz = bsz + 1;
	                assert(bucketSzs[(size_t)i] == 0 || bucketReps[(size_t)i] != OFF_MASK);
	                if(i < numBuckets-1) {
	                    mergedSz = bucketSzs[(size_t)i] + bucketSzs[(size_t)i+1] + 1;
	                }
	                // Merge?
	                if(mergedSz <= bsz) {
	                    bucketSzs[(size_t)i+1] += (bucketSzs[(size_t)i]+1);
	                    // The following may look strange, but it's necessary
	                    // to ensure that the merged bucket has a representative
	                    bucketReps[(size_t)i+1] = _sampleSuffs[(size_t)i+added];
	                    _sampleSuffs.erase((size_t)i+added);
	                    bucketSzs.erase((size_t)i);
	                    bucketReps.erase((size_t)i);
	                    i--; // might go to -1 but ++ will overflow back to 0
	                    numBuckets--;
	                    merged++;
	                    assert_eq(numBuckets, _sampleSuffs.size()+1-added);
	                    assert_eq(numBuckets, bucketSzs.size());
	                }
	                // Split?
	                else if(bucketSzs[(size_t)i] > bsz) {
	                    // Add an additional sample from the bucketReps[]
	                    // set accumulated in the binarySASearch loop; this
	                    // effectively splits the bucket
	                    _sampleSuffs.insert(bucketReps[(size_t)i], (TIndexOffU)(i + (added++)));
	                }
	            }
	        }
	        if(added == 0) {
	            //if(this->verbose()) {
	            //	cout << "Final bucket sizes:" << endl;
	            //	cout << "  (begin): " << bucketSzs[0] << " (" << (int)(bsz - bucketSzs[0]) << ")" << endl;
	            //	for(uint32_t i = 1; i < numBuckets; i++) {
	            //		cout << "  " << bucketSzs[i] << " (" << (int)(bsz - bucketSzs[i]) << ")" << endl;
	            //	}
	            //}
	            break;
	        }
	        // Otherwise, continue until no more buckets need to be
	        // split
	        VMSG_NL("Split " << added << ", merged " << merged << "; iterating...");
	    }
	    // Do *not* force a do-over
	    //	if(limit == 0) {
	    //		VMSG_NL("Iterated too many times; trying again...");
	    //		buildSamples();
	    //	}
	    VMSG_NL("Avg bucket size: " << ((double)(len-_sampleSuffs.size()) / (_sampleSuffs.size()+1)) << " (target: " << bsz << ")");
	}
	
	private static long suffixLcp(T t, long aOff, long bOff) {
		TIndexOffU c = 0;
		size_t len = t.length();
		while(aOff + c < len && bOff + c < len && t[aOff + c] == t[bOff + c]) c++;
		return c;
	}
	
	private boolean tieBreakingLcp(long aOff, long bOff, long lcp, boolean lcpIsSoft) {
		const TStr& t = this->text();
		TIndexOffU c = 0;
		TIndexOffU tlen = (TIndexOffU)t.length();
		uint32_t dcDist = _dc.get()->tieBreakOff(aOff, bOff);
		lcpIsSoft = false; // hard until proven soft
		while(c < dcDist &&    // we haven't hit the tie breaker
		      c < tlen-aOff && // we haven't fallen off of LHS suffix
		      c < tlen-bOff && // we haven't fallen off of RHS suffix
		      t[aOff+c] == t[bOff+c]) // we haven't hit a mismatch
			c++;
		lcp = c;
		if(c == tlen-aOff) {
			// Fell off LHS (a), a is greater
			return false;
		} else if(c == tlen-bOff) {
			// Fell off RHS (b), b is greater
			return true;
		} else if(c == dcDist) {
			// Hit a tie-breaker element
			lcpIsSoft = true;
			return _dc.get()->breakTie(aOff+c, bOff+c) < 0;
		} else {
			return t[aOff+c] < t[bOff+c];
		}
	}
	
	public static long lookupSuffixZ(T t, long zOff, long off, EList<Long> z) {
		if(zOff < z.size()) {
			TIndexOffU ret = z[zOff];
			return ret;
		}
		return suffixLcp(t, off + zOff, off);
	}
	
	public boolean suffixCmp(long cmp, long i, long j, long k, boolean kSoft, EList<Long> z) {
		const TStr& t = this->text();
		TIndexOffU len = (TIndexOffU)t.length();
		// i is not covered by any previous match
		TIndexOffU l;
		if((int64_t)i > k) {
			k = i; // so that i + lHi == kHi
			l = 0; // erase any previous l
			kSoft = false;
			// To be extended
		}
		// i is covered by a previous match
		else /* i <= k */ {
			assert_gt((int64_t)i, j);
			TIndexOffU zIdx = (TIndexOffU)(i-j);
			assert_leq(zIdx, len-cmp);
			if(zIdx < _dcV || _dc.get() == NULL) {
				// Go as far as the Z-box says
				l = lookupSuffixZ(t, zIdx, cmp, z);
				if(i + l > len) {
					l = len-i;
				}
				assert_leq(i + l, len);
				// Possibly to be extended
			} else {
				// But we're past the point of no-more-Z-boxes
				bool ret = tieBreakingLcp(i, cmp, l, kSoft);
				// Sanity-check tie-breaker
				if(this->sanityCheck()) {
					if(ret) assert(sstr_suf_lt(t, i, t, cmp, false));
					else    assert(sstr_suf_gt(t, i, t, cmp, false));
				}
				j = i;
				k = i + l;
				if(this->sanityCheck()) {
					if(kSoft) { assert_leq(l, suffixLcp(t, i, cmp)); }
					else      { assert_eq (l, suffixLcp(t, i, cmp)); }
				}
				return ret;
			}
		}

		// Z box extends exactly as far as previous match (or there
		// is neither a Z box nor a previous match)
		if((int64_t)(i + l) == k) {
			// Extend
			while(l < len-cmp && k < (int64_t)len && t[(size_t)(cmp+l)] == t[(size_t)k]) {
				k++; l++;
			}
			j = i; // update furthest-extending LHS
			kSoft = false;
			assert_eq(l, suffixLcp(t, i, cmp));
		}
		// Z box extends further than previous match
		else if((int64_t)(i + l) > k) {
			l = (TIndexOffU)(k - i); // point to just after previous match
			j = i; // update furthest-extending LHS
			if(kSoft) {
				while(l < len-cmp && k < (int64_t)len && t[(size_t)(cmp+l)] == t[(size_t)k]) {
					k++; l++;
				}
				kSoft = false;
				assert_eq(l, suffixLcp(t, i, cmp));
			} else assert_eq(l, suffixLcp(t, i, cmp));
		}

		// Check that calculated lcp matches actual lcp
		if(this->sanityCheck()) {
			if(!kSoft) {
				// l should exactly match lcp
				assert_eq(l, suffixLcp(t, i, cmp));
			} else {
				// l is an underestimate of LCP
				assert_leq(l, suffixLcp(t, i, cmp));
			}
		}
		assert_leq(l+i, len);
		assert_leq(l, len-cmp);

		// i and cmp should not be the same suffix
		assert(l != len-cmp || i+l != len);

		// Now we're ready to do a comparison on the next char
		if(l+i != len && (
		   l == len-cmp || // departure from paper algorithm:
		                   // falling off pattern implies
		                   // pattern is *greater* in our case
		   t[i + l] < t[cmp + l]))
		{
			// Case 2: Text suffix is less than upper sample suffix
	#ifndef NDEBUG
			if(this->sanityCheck()) {
				assert(sstr_suf_lt(t, i, t, cmp, false));
			}
	#endif
			return true; // suffix at i is less than suffix at cmp
		}
		else {
			// Case 3: Text suffix is greater than upper sample suffix
	#ifndef NDEBUG
			if(this->sanityCheck()) {
				assert(sstr_suf_gt(t, i, t, cmp, false));
			}
	#endif
			return false; // suffix at i is less than suffix at cmp
		}
	}
	
	public void qsort(EList<Long> bucket) {
		String t = this.text();
		TIndexOffU *s = bucket.ptr();
		size_t slen = bucket.size();
		TIndexOffU len = (TIndexOffU)t.length();
		if(_dc.get() != NULL) {
			// Use the difference cover as a tie-breaker if we have it
			VMSG_NL("  (Using difference cover)");
			// Extract the 'host' array because it's faster to work
			// with than the EList<> container
			const uint8_t *host = (const uint8_t *)t.buf();
			assert(_dc.get() != NULL);
			mkeyQSortSufDcU8(t, host, len, s, slen, *_dc.get(), 4,
			                 this->verbose(), this->sanityCheck());
		} else {
			VMSG_NL("  (Not using difference cover)");
			// We don't have a difference cover - just do a normal
			// suffix sort
			mkeyQSortSuf(t, s, slen, 4,
			             this->verbose(), this->sanityCheck());
		}
	}
}
